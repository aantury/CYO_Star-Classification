---
title: "CYO - Star Classification"
author: "Andres Antury"
date: "11/02/2022"
output: pdf_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Executive summary 


The aim of this project is to create a start classification system. The dataset used for this project is a 6-class star dataset for star classification which was sourced from *(https://www.kaggle.com/deepu1109/star-dataset/metadata)*. The dataset was licensed by its author Deepraj Baidya. The dataset included the following variables:

* Absolute Temperature (in K) consisting on the surface temperature of different stars
* Relative Luminosity (L/Lo) of the starts compared with the Sun
* Radius (R/Ro) of the starts compared with the Sun
* Absolute Visual Magnitude of Stars
* Star Type which is divided in 6 classes *(Brown Dwarf, Red Dwarf, White Dwarf, Main Sequence star, Supergiant and Hypergiant)*
* Star Color obtained following spectral analysis
* Spectral Class which is divided in 6 classes *(O,B,A,F,G,K,M)*  

The process included data cleansing, data exploration as well as feature selection to ensure only relevant data was used for the different algorithms used.

In order to create the Machine Learning algorithm for the classification system, the star dataset was split using the Caret package to create the the training and test sets. 
The evaluation metrics used to achieve the best model were derived using the accuracy which is developed using the Confusion Matrix. *(Irizarri, R. Introduction to Data Science, p. 506)*.
 
The R packages used for this project included *"tidyverse"*, *"caret"*, *"data.table"*, *"randomForest"*, *"nnet"*, *"party"*, and *"earth"*.

The models compared were trained using the caret package and included *Random Forest*, *Conditional Inference Random Forest*, *Bagged Multivariate Adaptive Regression Splines*, and *Penalized Multinomial Regression*.

A comparison table was created to view the different accuracy values achieved by the models trained with the train set and then tested with the test set.

\newpage

## 2. Analysis

For the analysis, the data set is downloaded from the original repository and the csv file saved in the personal Github repository of the author of this report which can be found on *(https://raw.githubusercontent.com/aantury/CYO_Star-Classification/main/6_class.csv*. The Dataset is retrieved from the Github repository using the methodology introduced during the course *(Irizarri, R. Introduction to Data Science, p. 109)*

The 6_class.csv dataset once downloaded into the R script is then converted to a data.frame and renamed as "start_dat". 


```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, results='hide'}
library(tidyverse)
library(caret)
library(data.table)
url <- "https://raw.githubusercontent.com/aantury/CYO_Star-Classification/main/6_class.csv"
tmp_filename <- tempfile()
download.file(url, tmp_filename)
star_dat <- read_csv(tmp_filename, show_col_types = FALSE) 
file.remove(tmp_filename)
```

### 2.1 Data exploration and data cleansing 
Data exploration and data cleansing were conducted in order for the data to be formatted in a way that allowed better interpretation and usability.

To view the top 6 rows of star_dat dataset and the column names we use the "head" function on the dataset.
```{r comment = " "}
head(star_dat)
```

Using the "nrow" function we can confirm that the number of observations (rows) in the star_dat dataset is: 
```{r echo=FALSE, comment = " "}
nrow(star_dat)
```

Star_type has nominal numbers assigned instead of descriptive names. These will be replaced by the actual star type for better interpretation e.g Star type 0 = Brown Dwarf, etc., as specified in the original dataset repository using the following code (only 1 star type shown):
```{r comment = " ", warning=FALSE, error=FALSE, message=FALSE, results="hide"}
star_dat$Star_type <- as.character(star_dat$Star_type)
star_dat["Star_type"][star_dat["Star_type"] == 0] <- "Brown_Dwarf"
```
```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, results="hide"}
star_dat["Star_type"][star_dat["Star_type"] == 1] <- "Red_Dwarf"
star_dat["Star_type"][star_dat["Star_type"] == 2] <- "White_Dwarf"
star_dat["Star_type"][star_dat["Star_type"] == 3] <- "Main_Sequence"
star_dat["Star_type"][star_dat["Star_type"] == 4] <- "Supergiant"
star_dat["Star_type"][star_dat["Star_type"] == 5] <- "Hypergiant" 
```

The dataset structure can be viewed using the "str" function and it can be seen that Temperature, Luminosity, Radius and Absolute magnitude are numeric while Star color, Spectral Class and Star type are character as expected.  
```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, results="hide"}
str(star_dat)
```

To find the number of occurrences of different star colors we use the "table" function. This gives us the ranking of the colors according to the number of times they appear in the dataset
```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, comment = " "}
Rank_1 <- table(star_dat$Star_color)
Rank_1 %>% as.data.frame() %>% 
  arrange(desc(Freq))
```

We can see that the Star_color column has duplicate entries for the same color (with slight differences in spelling), those repeated colors are be merged and presented with the same formatting using the following code (only 1 color type shown):
```{r comment = " ", warning=FALSE, error=FALSE, message=FALSE, results="hide"}
star_dat[star_dat == "Blue-white" | star_dat == "Blue White"| star_dat == "Blue white" | 
           star_dat == "Blue-White" ] <- "Blue_White"
```
```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, results="hide"}
star_dat[star_dat == "yellow-white"  ] <- "Yellow_White"
star_dat[star_dat == "white" ] <- "White"
star_dat[star_dat == "yellowish" ] <- "Yellowish"
star_dat[star_dat == "Orange-Red" ] <- "Orange_Red"
star_dat[star_dat == "Pale yellow orange" ] <- "Pale_Yellow_Orange"
star_dat[star_dat == "Yellowish White" ] <- "Yellowish_White"
star_dat[star_dat == "White-Yellow" ] <- "White_Yellow" 
```

Similarly, to obtain number of occurrences of the different spectral classes we use the "table" function. We can see that the Spectral class most frequent in the dataset is the spectral class M
```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, comment = " "}
Rank2 <- table(star_dat$Spectral_class) 
Rank2 %>% as.data.frame() %>% 
  arrange(desc(Freq))
```

Converting the character columns to factors. This is important for the analysis of the models to be used later on. The methodology was adapted from *(https://michaelbarrowman.co.uk/post/convert-all-character-variables-to-factors/)*
```{r comment = " ", warning=FALSE, error=FALSE, message=FALSE}
star_dat <- star_dat %>% mutate(across(where(is.character),as_factor))

head(star_dat)

```

### 2.2 Data Visualization

To view the different features plotted against Star type and wrapped around Spectral Class (M,B,A,F,O,K,G), we can use the boxplot diagram as below:

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, comment=FALSE,  out.width="75%", fig.align = 'center'}
star_dat %>% ggplot(aes(x = Star_type, y = Temperature, fill = Star_type)) + geom_boxplot() +
  facet_wrap(~Spectral_class, scales = "free", ncol = 4)+
  theme(axis.text.x = element_blank(), legend.position="bottom") 
```

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, comment=FALSE,  out.width="75%", fig.align = 'center'}
star_dat %>% ggplot(aes(x = Star_type, y = Luminosity, fill = Star_type)) + geom_boxplot() +
  facet_wrap(~Spectral_class, scales = "free", ncol = 4) +
  theme(axis.text.x = element_blank(), legend.position="bottom")
```

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, comment=FALSE,  out.width="75%", fig.align = 'center'}
star_dat %>% ggplot(aes(x = Star_type, y = Radius, fill = Star_type)) + geom_boxplot() +
  facet_wrap(~Spectral_class, scales = "free", ncol = 4) +
  theme(axis.text.x = element_blank(), legend.position="bottom")
```

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, comment=FALSE,  out.width="75%", fig.align = 'center'}
star_dat %>% ggplot(aes(x = Star_type, y = Absolute_magnitude, fill = Star_type)) + 
  geom_boxplot() +
  facet_wrap(~Spectral_class, scales = "free", ncol = 4) +
  theme(axis.text.x = element_blank(), legend.position="bottom")
```

We can see that spectral class G gives very little information and this could be due to the very small number sampled for this class. it can also be seen that the radius of the Hypergiants is considerably larger than the other types as expected. The boxplot allows to identify the most relevant features per star type and per spectral class. Further information regarding the relationship between the different features can be seen later in the correlation analysis.

### 2.3 Feature Selection

#### 2.3.1 Correlation 
a. Converting the star_dat dataset into a matrix. Only the continuous features are selected.

```{r warning=FALSE, error=FALSE, message=FALSE, results="hide"}
star_dat_cM <- star_dat %>% select(Temperature, Luminosity, Radius, 
                                   Absolute_magnitude)%>% as.matrix() 
```

b. Creation of correlation matrix using the "cor" function. Only showing 2 decimals for ease of viewing.

```{r warning=FALSE, error=FALSE, message=FALSE}
CM <- round(cor(star_dat_cM), 2)
```
```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, comment = " "}
CM
```

c. To see whether objects are highly correlated and what are they, we use the "findCorrelation" function.

```{r warning=FALSE, error=FALSE, message=FALSE}
high_corr <- findCorrelation(CM, cutoff=0.5, exact = TRUE)
```
```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, comment = " "}
high_corr
```
As can be seen from the table, both Luminosity (2) and Absolute Magnitude (4) are highly correlated with each other, therefore, these features should not be used for analysis.

The features that will be used as predictors are: **Temperature**, **Radius**, **Star Color** and **Spectral Class**. The target feature is **Star type**

#### 2.3.2 Near-zero Variance
In addition to the correlation, it is useful to check whether any of the  features in de dataset have near Zero variance. It is important to find and remove any zero or near-zero variance features as they add no value to the analysis. In this case the "nearZeroVar" function from the caret package is used. Methodology used was obtained from *(https://www.rdocumentation.org/packages/caret/versions/6.0-90/topics/nearZeroVar)*

```{r warning=FALSE, error=FALSE, message=FALSE}
star_dat_nzv <- nearZeroVar(star_dat, saveMetrics = TRUE)
```
```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, comment = " "}
star_dat_nzv
```

Using the "is.na" function in the star_dat data set, it can be confirmed that there are none of the features within the dataset have any NA values.

### 2.4 Train and Test sets

The methodology introduced during the course will be used to obtain the test and train datasets for this CYO project. The "createDataPartition" function of the "caret" package is used to create the train and test sets.

The Test set will be 20% of star_dat dataset. This percentage of partition was chosen because the star_dat dataset is not very large as there are 240 observations.

```{r warning=FALSE, error=FALSE, message=FALSE}
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(y = star_dat$Star_type, times = 1, p = 0.2, list = FALSE)
train_data <- star_dat[-test_index,]      
test_data <- star_dat[test_index,] 
```

### 2.5 Machine Learning Algorithms Training and Testing

For this project, the comparison between different machine learning models will be using the "train" function of the caret package. The target feature is the **Star_type**, therefore, the Star Type on the test set needs to be converted to factor in order to be used to test the models.

```{r warning=FALSE, error=FALSE, message=FALSE}
test_data$Star_type <- as.factor(test_data$Star_type)
```

As explained earlier, the predictors to be used are Temperature, Radius, Star Color and Spectral Class.

The modeling techniques used for this project were: **Random Forest**, **Conditional Inference Random Forest**, **Bagged Multivariate Adaptive Regression Splines (MARS)**, and **Penalized Multinomial Regression**.

The methodology used for the different models was based on the caret documentation found on *(https://topepo.github.io/caret/train-models-by-tag)*. 

#### 2.5.1 Random Forest using caret method "rf" and the "randomForest" package as explained in 
*(https://topepo.github.io/caret/train-models-by-tag.html#Random_Forest)*

a. Training the model using default "trainControl" option

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, results='hide'}
library(randomForest)
set.seed(1, sample.kind="Rounding")
```
```{r warning=FALSE, error=FALSE, message=FALSE}
star_rf <- train(Star_type ~ Temperature + Radius +  
                           Star_color + Spectral_class, method = "rf", data=train_data, 
                         metric = "Accuracy", trControl = trainControl()) 
```

b. Model prediction using the "predict" function on the test_data set

```{r warning=FALSE, error=FALSE, message=FALSE}
star_prediction_rf <- predict(star_rf, test_data) 
```

c. Accuracy as per the Confusion Matrix

```{r warning=FALSE, error=FALSE, message=FALSE, results='hide'}
confusionMatrix(star_prediction_rf, test_data$Star_type)$overall[["Accuracy"]]
```

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, results='hide'}
#Accuracy table
Accuracy_results <- tibble(Method = "a. Random Forest", Accuracy = 
                                       confusionMatrix(star_prediction_rf, 
                                                       test_data$Star_type)$overall[["Accuracy"]])
Accuracy_results %>% knitr::kable()
```

#### 2.5.2 Conditional Inference Random Forest. Implementation of the random forest and bagging ensemble using the using caret method "cforest" and the "party" package found in *(https://www.rdocumentation.org/packages/partykit/versions/1.2-15/topics/cforest)* as explained in *(https://topepo.github.io/caret/train-models-by-tag.html)*

a. Training the model using the default train control values in "cforest_control"

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, results='hide'}
library(party)
set.seed(1, sample.kind="Rounding")
```
```{r warning=FALSE, error=FALSE, message=FALSE}
star_cforest <- train(Star_type ~ Temperature + Radius +  
                      Star_color + Spectral_class, method = "cforest", 
                      data=train_data, metric = "Accuracy", controls = cforest_control() )
```

b. Model prediction using the "predict" function on the test_data set

```{r warning=FALSE, error=FALSE, message=FALSE}
star_prediction_cforest <- predict(star_cforest, test_data) 
```

c. Accuracy as per the Confusion Matrix

```{r warning=FALSE, error=FALSE, message=FALSE, results='hide'}
confusionMatrix(star_prediction_cforest, test_data$Star_type)$overall[["Accuracy"]]
```

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, results='hide'}
#Accuracy table
Accuracy_results <- bind_rows(Accuracy_results,
                              tibble(Method = "b. Conditional Inference Random Forest", Accuracy = 
                                       confusionMatrix(star_prediction_cforest, 
                                                       test_data$Star_type)$overall[["Accuracy"]]))
Accuracy_results %>% knitr::kable()
```

#### 2.5.3 Bagged Multivariate Adaptive Regression Splines (MARS) using using caret method "bagEarth" and the "earth" package found in
*(https://www.rdocumentation.org/packages/caret/versions/6.0-90/topics/bagEartha)* as explained in *(https://topepo.github.io/caret/train-models-by-tag.html)*

a. Training the model setting "glm option = null" as this is not applicable on this model and would otherwise affect the result

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, results='hide'}
library(earth)
set.seed(1, sample.kind="Rounding")
```
```{r warning=FALSE, error=FALSE, message=FALSE}
star_bagEarth <- train(Star_type ~ Temperature + Radius +  
                       Star_color + Spectral_class, method = "bagEarth", 
                       data=train_data, metric = "Accuracy", glm = NULL) 
```

b. Model prediction using the "predict" function on the test_data set

```{r warning=FALSE, error=FALSE, message=FALSE}
star_prediction_bagEarth <- predict(star_bagEarth, test_data)
```

c. Accuracy as per the Confusion Matrix

```{r warning=FALSE, error=FALSE, message=FALSE, results='hide'}
confusionMatrix(star_prediction_bagEarth, test_data$Star_type)$overall[["Accuracy"]]
```

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, results='hide'}
#Accuracy table
Accuracy_results <- bind_rows(Accuracy_results,
                              tibble(Method = "c. Bagged MARS Model", Accuracy = 
                                       confusionMatrix(star_prediction_bagEarth, 
                                                       test_data$Star_type)$overall[["Accuracy"]]))
Accuracy_results %>% knitr::kable()
```

#### 2.5.4 Neural network with principal component step using caret method "multinom" and the "nnet" package and the penalized multinomial regression found in *(https://www.rdocumentation.org/packages/nnet/versions/7.3-17/topics/multinom)*as explained in *(https://topepo.github.io/caret/train-models-by-tag.html)*

a. Training the model setting the "maxit" parameter to 1000 iterations in order for the model to converge as the default 100 iterations are not sufficient

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, results='hide'}
library(nnet)
set.seed(1, sample.kind="Rounding")
```
```{r warning=FALSE, error=FALSE, message=FALSE, results='hide'}
star_pmr <- train(Star_type ~ Temperature + Radius +  
                  Star_color + Spectral_class, method = "multinom", data=train_data, 
                  maxit = 1000) 
```

b. Model prediction using the "predict" function on the test_data set

```{r warning=FALSE, error=FALSE, message=FALSE, results='hide' }
star_prediction_pmr <- predict(star_pmr, test_data)
```

c. Accuracy as per the Confusion Matrix

```{r warning=FALSE, error=FALSE, message=FALSE, results='hide'}
confusionMatrix(star_prediction_pmr, test_data$Star_type)$overall[["Accuracy"]]
```

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, results='hide'}
#Accuracy table
Accuracy_results <- bind_rows(Accuracy_results,
                              tibble(Method = "d. Penalized multinomial regression", Accuracy = 
                                       confusionMatrix(star_prediction_pmr, 
                                                       test_data$Star_type)$overall[["Accuracy"]]))
Accuracy_results %>% knitr::kable()
```

\newpage

## 3. Results

The following table shows the accuracy achieved by the models trained and tested and shows the accuracy achieved by  the best performing model which was the one using the **Penalized Multinomial Regression with neural network**, the accuracy achieved was **0.9583**

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
Accuracy_results %>% knitr::kable()
```

\newpage

## 4. Conclusion

### 4.1 Summary  

The project consisted of 3 main parts: 

#### 4.1.1 Data Exploration and Dataset Creation  
This step included the review of the star classification dataset. The Datasets necessary for the data analysis and Machine Learning model creation were obtained from the start_dat dataset. The datasets created included a partition of the start_dat dataset to create the training and test sets. 

#### 4.1.2 Data Analysis  
Following the creation of the training and test datasets, 4 different models were trained and tested using different techniques including the implementation of the Random Forest and bagging Ensemble, Multivariate Adaptive Regression Splines and Penalized Multinomial Regression. 
The evaluation metrics used for the models was the accuracy which was obtained from the confusiuon matrix. The accuracy was used to compare the models and to select the best performing model.

#### 4.1.3 Best performing model selection  
The best performing model based on the test set was the one using **Penalized Multinomial Regression** with Neural Networks which was modeled using the "nnet" package and the "multinom" function and trained in the "caret" package. This model then achieved an accuracy of **0.9583**, leading to a satisfactory result.

### 4.2 Limitations 

Due to the size of the original dataset, the models evaluated could be trained using the "train" function of the "caret" package and using in most cases default parameters. The main driver for this was the preference of the author of this report to provide simple and less complex models. 

The main objective has been to reach the goal of selecting the best performing model among the different Machine Learning modeling techniques, and the application of different methodologies learned during the whole data science course. 

Should larger datasets become necessary, the models could take a considerable amount of time to run, so it is preferable to limit the size of the datasets to a level similar to the one used for this project if using the "train" function of the "caret" package to train the models. 

Alternatively, more complex neural networks based models could be used in larger datasets if required.

### 4.3 Future work  

It would be beneficial to improve the models evaluated so more tuning parameters are used specially on the Penalized Multinomial Regression and the Multivariate Adaptive Regression Splines. This could lead to more robust results and to enable the use of even larger and more complex datasets.  

Although there is room for improvement regarding the models, the author of this report believed that the main objective of the project has been achieved and the final result met the expectations. 